{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNs77v0l5wFo+Ji9ZfDZe5+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lior5egal/Deep-Learning-0512-436201/blob/main/HW1/EX_1_DL_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Theory**\n",
        "---\n"
      ],
      "metadata": {
        "id": "YBjAV8TR5Exs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 1**\n",
        "---\n",
        "### a. What is the shape of the input $ X $?\n",
        "The input $ X $ is a batch of size $ m $ with 10 features per sample. Therefore, its shape is:\n",
        "$ X \\in \\mathbb{R}^{m \\times 10} $\n",
        "\n",
        "### b. What about the shape of the hidden layer's weight vector $ W_h $, and the shape of its bias vector $ b_h $?\n",
        "- $ W_h $: This weight matrix maps the 10 input features to 50 hidden layer neurons. Its shape is:\n",
        "$ W_h \\in \\mathbb{R}^{10 \\times 50} $\n",
        "- $ b_h $: This bias vector is added to the 50 neurons in the hidden layer. Its shape is:\n",
        "$ b_h \\in \\mathbb{R}^{50} $\n",
        "\n",
        "### c. What is the shape of the output layer's weight vector $ W_o $, and its bias vector $ b_o $?\n",
        "- $ W_o $: This weight matrix maps the 50 hidden neurons to 3 output neurons. Its shape is:\n",
        "$ W_o \\in \\mathbb{R}^{50 \\times 3} $\n",
        "- $ b_o $: This bias vector is added to the 3 output neurons. Its shape is:\n",
        "$ b_o \\in \\mathbb{R}^{3} $\n",
        "\n",
        "### d. What is the shape of the network's output matrix $ Y $?\n",
        "The output matrix $ Y $ contains the predictions for $ m $ samples, each with 3 output values. Its shape is:\n",
        "$ Y \\in \\mathbb{R}^{m \\times 3} $\n",
        "\n",
        "### e. Write the equation that computes the network's output matrix $ Y $ as a function of $ X, W_h, b_h, W_o, $ and $ b_o $.\n",
        "The computation of the output $ Y $ involves the following steps:\n",
        "1. Compute the pre-activation for the hidden layer:\n",
        "   $ Z_h = X W_h + b_h $\n",
        "2. Apply the ReLU activation function:\n",
        "   $ H = \\text{ReLU}(Z_h) = \\max(0, Z_h) $\n",
        "3. Compute the pre-activation for the output layer:\n",
        "   $ Z_o = H W_o + b_o $\n",
        "4. (Optional) Apply a non-linear activation to $ Z_o $, if specified, such as softmax for classification.\n",
        "\n",
        "In an MLP, the right thing to do for the output layer depends on the specific task we are solving.\n",
        "\n",
        "**1. Regression Tasks**\n",
        "\n",
        "If the goal is to predict real-valued outputs (e.g., house prices, temperatures):\n",
        "- **Output layer activation**: **No activation function** (linear activation).  \n",
        "  This allows the network to output any value in the real number range, positive or negative.\n",
        "- **Loss function**: Mean Squared Error (MSE) or Mean Absolute Error (MAE). $Y = X W_h + b_h$\n",
        "**2. Binary Classification (Two Classes)**\n",
        "If the goal is to classify inputs into one of two classes (e.g., spam vs. not spam):\n",
        "- **Output layer activation**: **Sigmoid**.  \n",
        "  This compresses the output to the range $[0, 1]$, making it interpretable as a probability for one class.\n",
        "- **Loss function**: Binary Cross-Entropy.\n",
        "\n",
        "$$\n",
        "Y = \\text{Sigmoid}(X W_h + b_h) = \\frac{1}{1 + e^{-(X W_h + b_h)}}\n",
        "$$\n",
        "\n",
        "**3. Multi-Class Classification (More than Two Classes)**\n",
        "If the goal is to classify inputs into one of multiple categories (e.g., dog, cat, bird):\n",
        "- **Output layer activation**: **Softmax**.  \n",
        "  This ensures that the output values are probabilities for each class, summing to 1.\n",
        "- **Loss function**: Categorical Cross-Entropy.\n",
        "\n",
        "$$\n",
        "Y = \\text{Softmax}(Z_o) = \\frac{e^{Z_o^{(i)}}}{\\sum_{j} e^{Z_o^{(j)}}}\n",
        "$$\n",
        "\n",
        "Where $ Z_o $ is the output before activation, and $ i, j $ are class indices.\n",
        "\n",
        "**4. Multi-Label Classification**\n",
        "If the goal is to predict multiple independent binary labels for each input (e.g., predicting attributes like \"male\" and \"smiling\" for a face image):\n",
        "- **Output layer activation**: **Sigmoid** for each output neuron.\n",
        "- **Loss function**: Binary Cross-Entropy for each label.\n",
        "\n",
        "$$\n",
        "Y = \\text{Sigmoid}(X W_h + b_h)\n",
        "$$\n",
        "\n",
        "**5. Specialized Tasks**\n",
        "For other tasks (e.g., energy functions, constrained outputs), you might use custom activation functions or modifications, such as:\n",
        "- **ReLU** for ensuring non-negative outputs (e.g., counting objects).\n",
        "- **Tanh** if the outputs must be in the range $[-1, 1]$.\n",
        "\n",
        "---\n",
        "\n",
        "### **General Rule**\n",
        "- **For the Output Layer**: Choose the activation function based on the task requirements:\n",
        "  - **None** for regression.\n",
        "  - **Sigmoid** for binary classification.\n",
        "  - **Softmax** for multi-class classification.\n",
        "\n",
        "By tailoring the output layer's activation to the task, the MLP will produce outputs appropriate for the problem we're solving\n",
        "\n",
        "Therefore, the equation for $ Y $ is:\n",
        "\n",
        "\n",
        "$ Y = Φ(\\text{ReLU}(X W_h + b_h) W_o + b_o) $\n",
        "\n",
        "\n",
        "Where $ Φ $ is the activation function of the output layer"
      ],
      "metadata": {
        "id": "x5F04QzI2w3K"
      }
    }
  ]
}