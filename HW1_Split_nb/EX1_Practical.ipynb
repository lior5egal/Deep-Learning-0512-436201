{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Practical**\n",
    "This notebook implements a Convolutional Neural Network (CNN) in PyTorch for image classification using the FashionMNIST dataset.\n",
    "\n",
    "---\n",
    "### Overview:\n",
    "- Importing necessary libraries.\n",
    "- Loading and preprocessing the dataset.\n",
    "- Defining CNN architectures with variations.\n",
    "- Training and evaluating the models.\n",
    "- Visualizing the training and testing accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Importing Libraries**\n",
    "The necessary libraries for building and training the CNN are imported. We use:\n",
    "- `torch` for creating and training neural networks.\n",
    "- `numpy` for numerical computations.\n",
    "- `matplotlib` for visualization.\n",
    "- `torchvision` for accessing datasets and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Check CUDA Availability**\n",
    "We check if a GPU is available for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Data Loading and Preprocessing**\n",
    "We define:\n",
    "- `num_workers`: Number of subprocesses for data loading.\n",
    "- `batch_size`: Number of samples per batch.\n",
    "- Transformations: Convert images to tensors and apply normalization.\n",
    "- Load the FashionMNIST dataset and shuffle training indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0 \n",
    "batch_size = 60\n",
    "\n",
    "train_transform = transforms.ToTensor()\n",
    "test_transform = transforms.ToTensor()\n",
    "\n",
    "train_data = datasets.FashionMNIST('FashionMNIST/raw/train-images-idx3-ubyte', train=True, download=True, transform=train_transform)\n",
    "test_data = datasets.FashionMNIST('FashionMNIST/raw/t10k-images-idx3-ubyte', train=False, download=True, transform=test_transform)\n",
    "\n",
    "# obtain training indices\n",
    "num_train = len(train_data)\n",
    "train_idx = list(range(num_train))\n",
    "np.random.shuffle(train_idx)\n",
    "\n",
    "# define samplers for obtaining training\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "classes = [\"T-shirt/top\", \\\n",
    "           \"Trouser\",\\\n",
    "           \"Pullover\",\\\n",
    "           \"Dress\",\\\n",
    "           \"Coat\",\\\n",
    "           \"Sandal\",\\\n",
    "           \"Shirt\",\\\n",
    "           \"Sneaker\",\\\n",
    "           \"Bag\",\\\n",
    "           \"Ankle boot\"\\\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Data Visualization**\n",
    "We display a batch of training images to visually confirm the data is loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)), cmap='gray')\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 10, idx + 1, xticks=[], yticks=[])\n",
    "    imshow(images[idx])\n",
    "    ax.set_title(classes[labels[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Defining CNN Architectures**\n",
    "We define three variations of CNN models with dropout, batch normalization, and without any modifications. These architectures are essential for comparing different regularization techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Base Class (`BaseNet`)\n",
    "\n",
    "The `BaseNet` class contains the core components that are shared across the different models. It defines:\n",
    "- Two convolutional layers (`conv1` and `conv2`).\n",
    "- An average pooling layer (`pool`).\n",
    "- Three fully connected layers (`fc1`, `fc2`, `fc3`).\n",
    "\n",
    "Additionally, it implements a `common_forward` method that processes the input through these layers in the forward pass. This method can be inherited by other models to reuse the common structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, padding=0)\n",
    "        self.pool = nn.AvgPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def common_forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Derived Models\n",
    "\n",
    "- **`Net_dropout`**: This model inherits from `BaseNet` and adds a dropout layer before the fully connected layers to prevent overfitting. Dropout is applied during training by randomly setting some of the activations to zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_dropout(BaseNet):\n",
    "    def __init__(self):\n",
    "        super(Net_dropout, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.common_forward(x)\n",
    "        x = self.dropout(x.view(-1, 84))  # Apply dropout before the fully connected layers\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`Net`**: A simpler model that directly uses the `common_forward` method from `BaseNet` without any additional modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(BaseNet):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.common_forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`Net_BatchNorm`**: This model also inherits from `BaseNet`, but it introduces batch normalization after each convolutional layer. Batch normalization helps improve the training of deep networks by normalizing the input to each layer, leading to faster convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_BatchNorm(BaseNet):\n",
    "    def __init__(self):\n",
    "        super(Net_BatchNorm, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(6)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        return self.common_forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Training Various CNN Models**\n",
    "After defining the models, we proceed with training them using different configurations. Each model is trained for 15 epochs with the following setup:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model Definition and Optimizer Setup\n",
    "We define the models in the `models` list, which includes:\n",
    "- `model_origin`: The basic model without dropout or batch normalization.\n",
    "- `model_dropout`: The model with dropout to prevent overfitting.\n",
    "- `model_batchnorm`: The model with batch normalization for improved training.\n",
    "- `model_weightdecay`: The basic model with added weight decay for regularization.\n",
    "\n",
    "For each model, we specify the loss function (`CrossEntropyLoss`) and the optimizer (`Adam`). In the case of `model_weightdecay`, weight decay is added to the optimizer to penalize large weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a various complete CNN\n",
    "model_origin = Net()\n",
    "model_dropout = Net_dropout()\n",
    "model_batchnorm = Net_BatchNorm()\n",
    "model_weightdecay = Net()\n",
    "\n",
    "models = [(model_origin,\"model_origin\"), (model_dropout,\"model_dropout\"),\n",
    "          (model_batchnorm,\"model_batchnorm\"),(model_weightdecay,\"model_weightdecay\")]\n",
    "\n",
    "print(model_origin)\n",
    "print(model_dropout)\n",
    "print(model_batchnorm)\n",
    "print(model_weightdecay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training Loop\n",
    "We train each model for 15 epochs. During each epoch, we:\n",
    "- Set the model to training mode.\n",
    "- Load the data and perform a forward pass to compute the output.\n",
    "- Calculate the loss and perform a backward pass to update the model parameters.\n",
    "- Track the training loss and save the model weights after each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the different models\n",
    "for i in range(4):\n",
    "\n",
    "  #defining the wanted model\n",
    "  model = models[i][0]\n",
    "  print(models[i][1])\n",
    "\n",
    "  # specify loss function (categorical cross-entropy)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  # specify optimizer\n",
    "  if i != 3:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "  else:\n",
    "    optimizer = optim.Adam(model.parameters(), weight_decay = 0.0002, lr=0.002)\n",
    "\n",
    "  # move tensors to GPU if CUDA is available\n",
    "  if train_on_gpu:\n",
    "      model.cuda()\n",
    "\n",
    "  # number of epochs to train the model\n",
    "  n_epochs = 15\n",
    "\n",
    "  for epoch in range(1, n_epochs+1):\n",
    "\n",
    "      # keep track of training and validation loss\n",
    "      train_loss = 0.0\n",
    "\n",
    "      ###################\n",
    "      # train the model #\n",
    "      ###################\n",
    "      model.train()\n",
    "      for data, target in train_loader:\n",
    "          # move tensors to GPU if CUDA is available\n",
    "          if train_on_gpu:\n",
    "              data, target = data.cuda(), target.cuda()\n",
    "          # clear the gradients of all optimized variables\n",
    "          optimizer.zero_grad()\n",
    "          # forward pass: compute predicted outputs by passing inputs to the model\n",
    "          output = model(data)\n",
    "          # calculate the batch loss\n",
    "          loss = criterion(output, target)\n",
    "          # backward pass: compute gradient of the loss with respect to model parameters\n",
    "          loss.backward()\n",
    "          # perform a single optimization step (parameter update)\n",
    "          optimizer.step()\n",
    "          # update training loss\n",
    "          train_loss += loss.item()*data.size(0)\n",
    "\n",
    "      # calculate average losses\n",
    "      train_loss = train_loss/len(train_loader.sampler)\n",
    "\n",
    "      # print training/validation statistics\n",
    "      print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "          epoch, train_loss))\n",
    "\n",
    "      # save model\n",
    "      torch.save(model.state_dict(), f'{models[i][1]}_{epoch}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Accuracy Evaluation of Each Model**\n",
    "\n",
    "After training the models, we evaluate their performance on both training and testing datasets. For each model:\n",
    "- We load the saved model weights from each epoch.\n",
    "- We calculate the accuracy on both the training and testing datasets.\n",
    "- We print the accuracy for each class and the overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of each model\n",
    "accuracy = []\n",
    "\n",
    "# Testing various models\n",
    "for idx in range(4):\n",
    "    model = models[idx][0]\n",
    "    print(f\"\\n{models[idx][1]}\\n\")\n",
    "\n",
    "    # List of accuracy per epoch with items: [train_accuracy, test_accuracy]\n",
    "    accuracy_model = []\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        print(f\"\\n*** Epoch number: {epoch} ***\\n\")\n",
    "        # Load the model\n",
    "        model.load_state_dict(torch.load(f'{models[idx][1]}_{epoch}.pt'))\n",
    "        # Change model into evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Specify loss function (categorical cross-entropy)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # List of current epoch's accuracies\n",
    "        epoch_accuracy = []\n",
    "\n",
    "        # Switch between the training and testing loaders\n",
    "        word_list = [\"Train\", \"Test\"]\n",
    "        for idx_word, testing_data in enumerate([train_loader, test_loader]):\n",
    "\n",
    "            ###################\n",
    "            # Test the model #\n",
    "            ###################\n",
    "            test_loss = 0.0\n",
    "            class_correct = list(0. for i in range(10))\n",
    "            class_total = list(0. for i in range(10))\n",
    "\n",
    "            for data, target in testing_data:\n",
    "                # Move tensors to GPU if CUDA is available\n",
    "                if train_on_gpu:\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "                # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "                output = model(data)\n",
    "                # Calculate the batch loss\n",
    "                loss = criterion(output, target)\n",
    "                # Update test loss\n",
    "                test_loss += loss.item()*data.size(0)\n",
    "                # Convert output probabilities to predicted class\n",
    "                _, pred = torch.max(output, 1)\n",
    "                # Compare predictions to true label\n",
    "                correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "                correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "                # Calculate test accuracy for each object class\n",
    "                for i in range(min(batch_size, len(target.data))):\n",
    "                    label = target.data[i]\n",
    "                    class_correct[label] += correct[i].item()\n",
    "                    class_total[label] += 1\n",
    "\n",
    "            # Average test loss\n",
    "            test_loss = test_loss/len(test_loader.dataset)\n",
    "            print(f'--- {word_list[idx_word]} Loss: {test_loss:.6f} ---\\n')\n",
    "\n",
    "            for i in range(10):\n",
    "                if class_total[i] > 0:\n",
    "                    print(f'{word_list[idx_word]} Accuracy of {classes[i]}: {100 * class_correct[i] / class_total[i]:2d}% ({int(np.sum(class_correct[i]))}/{int(np.sum(class_total[i]))})')\n",
    "                else:\n",
    "                    print(f'{word_list[idx_word]} Accuracy of {classes[i]}: N/A (no training examples)')\n",
    "\n",
    "            # Define the current overall accuracy and add it to \"epoch_accuracy\"\n",
    "            overall_accuracy = 100. * np.sum(class_correct) / np.sum(class_total)\n",
    "            epoch_accuracy.append(overall_accuracy)\n",
    "\n",
    "            print(f'\\n ### {word_list[idx_word]} Accuracy (Overall): {overall_accuracy:2d}% ({int(np.sum(class_correct))}/{int(np.sum(class_total))}) ###\\n')\n",
    "\n",
    "        # Adding epoch accuracies to \"accuracy_model\"\n",
    "        accuracy_model.append(epoch_accuracy)\n",
    "\n",
    "    # Adding accuracy_model to \"accuracy\"\n",
    "    accuracy.append(accuracy_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8. Plotting Model Accuracy**\n",
    "\n",
    "After evaluating the models, we plot the training and testing accuracies over all epochs for each model. This allows us to visualize the performance of each model during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating through models' accuracies\n",
    "for idx, model_acc in enumerate(accuracy):\n",
    "    print(models[idx][1])\n",
    "\n",
    "    # Extracting train and test accuracies\n",
    "    epoch_train_acc = [i[0] for i in model_acc]\n",
    "    epoch_test_acc = [i[1] for i in model_acc]\n",
    "\n",
    "    # Plotting accuracy over epochs\n",
    "    plt.plot([i for i in range(1, n_epochs + 1)], epoch_train_acc, label='train')\n",
    "    plt.plot([i for i in range(1, n_epochs + 1)], epoch_test_acc, label='test')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy [%]')\n",
    "    plt.title(f'Accuracy - {models[idx][1]}')\n",
    "    plt.show()\n",
    "\n",
    "    # Displaying final and maximum accuracies\n",
    "    print(f\"Final train accuracy: {epoch_train_acc[-1]}\")\n",
    "    print(f\"Final test accuracy: {epoch_test_acc[-1]}\\n\")\n",
    "    print(f\"Max train accuracy: {max(epoch_train_acc)} at epoch number {epoch_train_acc.index(max(epoch_train_acc))+1}\")\n",
    "    print(f\"Max test accuracy: {max(epoch_test_acc)} at epoch number {epoch_test_acc.index(max(epoch_test_acc))+1}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **9. Visualizing Model Predictions**\n",
    "\n",
    "Finally, we visualize the predictions of the models on a batch of test images. We show the true labels and predicted labels for each image, and color the labels green if the prediction is correct or red if it is incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain one batch of test images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Move model inputs to CUDA if GPU is available\n",
    "if train_on_gpu:\n",
    "    images = images.cuda()\n",
    "\n",
    "# Testing various models\n",
    "for i in range(3):\n",
    "    if i < 3:\n",
    "        # Defining the wanted model\n",
    "        model = models[i][0]\n",
    "        print(models[i][1])\n",
    "\n",
    "        # Get sample outputs\n",
    "        output = model(images)\n",
    "        # Convert output probabilities to predicted class\n",
    "        _, preds_tensor = torch.max(output, 1)\n",
    "        preds = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())\n",
    "\n",
    "        # Display test images with labels\n",
    "        for idx in range(6):\n",
    "            plt.subplot(2, 3, idx+1)\n",
    "            plt.imshow(images[idx].cpu().numpy().transpose((1, 2, 0)))\n",
    "            plt.title(f\"True: {classes[labels[idx]]}\\nPred: {classes[preds[idx]]}\", color=\"green\" if preds[idx] == labels[idx] else \"red\")\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
